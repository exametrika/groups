* GroupProtocolSubstack
* блокировка/разблокировка формирования членства группы в членстве кластера до момента завершения инсталляции ее локального членства через флаш
* уведомление о потере ланных при потере всех узлов группы
* составить план тестирования и реализовать тесты
* отладить мультикаст (используя симулятор) и починить тесты мультикаста
* фабрика каналов (составить список классов и их зависимостей по каждому каналу и как они будут вайрится)
* фабрика подстека группы, стратегия маппинга групп
* сборка проекта
* perf tests, distributed integration tests
----------
* юнит тест мультикастов не проходит
 - доделать проверку на ресивере при приеме сообщения, что все сообщения с тем же идентификатором приняты на других узлах, если да логгировать этот факт
  - проверять сколько послано и сколько принято
* дописать дефалтовые значения в GroupChannelFactory
* сделать юнит перф тест (взять за основу testMulticast посылать смаксимальной скоростью с учетом флоу контрол, исследовать зависимость пропускной способности и задержек от размера сообщения и параметров фабрики канала)
* Расширить StateTransfer Тесты - сделать тесты эмулирующие различные сбои при работе с состояние (reject, fail при загрузке/выгрузке и др.)
--------------------
Исследования на будущее:
* продумать изоляцию данных от кода, коллоцированных на одном хосте в одном (или разных узлах) с дешевым локальным взаимодействием между ними. Плюс - процессная изоляция, возможность использовать произвольную прикладную платформу (jvm, python, nodejs...)
* продумать про обновление кода - rolling updates, upfront поднятие доп. узла на том же хосте, спараллельным вводом его параллельно, без обновления данных (только код). Обновление частями кластера, например по зонам или по датацентрам
*  распределенный стриминг 
  - DAG обработка данных коллоцированная с самими данными, как узел графа - отдельный этап обработки
  - консистентные снимки, запись маркера в хранилище после определенного этапа обработки, с возможностью дальнейшего восстановления всего снимка для заданного маркера
  - использование окон данных для нарезки бесконечных потоков и обработка данных в рамках текущего окна
* дать возможность разделять на разных узлах узлы с данными и узлы без данных
* можно заменять имя потока на время выполнения запроса и восстанавливать после. Далее на проблемном запросе можно снять дамп потоков и увидеть всю диагностику. Аналогично можно использовать MDC или маркер для записи диагностики
----------
Делаем:
* cluster group, flush
* cluster state transfer, multicast
* поддержать для узлов и групп доп. свойства: роли, параметры производительности, метрики и др.
* вставить логи в там, где нужно
* code review и тесты всего, что сделано
* WorkerNodeChannel, CoreNodeChannel - каналы рабочего и core узлов
  - CoreClusterMembershipProtocol - должен быть выше по стеку чем мультикаст
  - для мультидоменной архитектуры - общий диспатчер и компартмент, разные подканалы со своим транспортом, ssl и менеджером коннектов
----------------------
«Искусство войны» Сунь Цзы, «Самурае без меча» Масао Китами и «Книге пяти колец» Миямото Мусаси. 

Подробнее на РБК:
http://www.rbc.ru/opinions/business/14/03/2017/58c7cc329a79470adfcec757?utm_source=right_11
----------
GroupProtocolSubstack:
* при установке/изменении членства группы в кластере (на узле координаторе группы):
  - если членство группы не установлено, устанавливает
  - если есть вышедшие/сбойные, формирует новое членство группы, исключив их (они исключаются также из  очереди на вход новых узлов, если они там есть)
  - если есть новые, запоминает список новых в очереди на вход (узлы должны входить списком все, иначе нарушится порядок следования узлов между членствами группы в кластере и в группе)
  - если в текуший момент идет флаш, формирование нового чоенства откоалывается (можно формирование сделать по таймеру, если текущий узел координатор группы, нет текущего флаша и есть сбойные/вышедшие (формируются при приходе членства кластера) или есть полностью заполненные входящие (голова очереди входящиз сравнивается с дрступными через дискаверер)
* периодически дергает IgroupnodeDiscoverer, проверяя доступны ли все узлы списка из головы очереди входящих, если да, включает их в новое членство  

* при падении рабочего узла будет сформировано новое членство кластера, где он будет помечен каксбойный/вышедший, в группе это будет расценено как сбой/выход, и он булет исключен из группы, все процессы  переноса состояния/входа будут отменены
* при падении координатора группы также будет сформировано новое членство группы и новый координатор продолжит формирование членств группы
* если на координаторе группы нет установленного членства группы, а группа не новая, дернуть стратегию потери данных, т.к. данные придеися тянуть из внешнего хранилища и все изменения не созраненные в нем теряются
