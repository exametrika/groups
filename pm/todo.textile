* юнит тест мультикастов не проходит
 - доделать проверку на ресивере при приеме сообщения, что все сообщения с тем же идентификатором приняты на других узлах, если да логгировать этот факт
  - проверять сколько послано и сколько принято
* дописать дефалтовые значения в GroupChannelFactory
* сделать юнит перф тест (взять за основу testMulticast посылать смаксимальной скоростью с учетом флоу контрол, исследовать зависимость пропускной способности и задержек от размера сообщения и параметров фабрики канала)
* Расширить StateTransfer Тесты - сделать тесты эмулирующие различные сбои при работе с состояние (reject, fail при загрузке/выгрузке и др.)
--------------
Base part:
* cluster membership и все что с ним связано (по аналонии с простым)
  - providers (подлержать node membership, workerToCore membership для начала)
* failure detector
  - worker (failure node history, фильтрация сообщений от сбойных, send shun сбойным и reconnect on shun, сбои формируются на узле по новому членству)
  - core 
    * worker node tracking strategy (маппинг берется из членства)
    * composite node tracking (для объединения с core group node tracking)
    * накапливает сбои и посылает core координатору, который формирует новое членство кластера 
    * список сбоев узла корректируется после инсталляции нового членства кластера (удаляя не вошелшие в новое членство)
    * узел ведет текущего координатора, перепосылая сбои в случае его смены
* мультитранспортная архитектура
  - необходима для задания периметров безопасности - формирует ssl периметр по каждому домену,  ограничивает состав сообщений от каждого домена (например, без этого клиент может послать shun core узлу и тот сложится)
  - основная проблема в - сейчас нельзя поддержать несколько транспортов в рамказ одного компартмента, т.к. у каждого свой диспетчер
  - идеальной была бы картина, когда имеем несколько стеков протоколов, каждый со своим транспортом в рамказ одного компартмента. Для этого нужно поддержать общий диспатчер, не зависящий от настроек конкретной ssl фабрики
  - тогда трекинг сбойных велся бы по каждому транспорту свой и не требовалась бы их композиция
  - и еще нужно не запутаться при посылке сообщений между каналами, нужно иметь в протоколе соотв. сендера, а не просто посылать в свой стек
  - сейчас такая возможность есть, просто создаем общего tcp диспетчера и компартмент в канале связи на все транспорты и их стек протоколов
--------------
* CoreClusterMembershipProtocol - должен быть выше по стеку чем мультикаст

Проблемы:
* в мультидоменной архитектуре узлу соответсвует более одного адреса, нужен способ получения правильного адреса в каждом из каналов, все адреса имеют одинаковый идентификатор, равный идентификатору узла
* распространять сбои воркеров сразу и без ожидания через текущего координатора кластера по всем узлам, коректировать при инсталляции членства
* генерировать новое членство кластера только после инсталляции предыдущего, воркеры шлют респонс по инсталляции, коре контроллер шлет координатору
* в алучае падения координатора или коре контроллера мезаника инсталляции членства и распространения сбойных блокируется, другой узел должен взять на себя досылку предыдущего недоинсталлированного членства или сбоев, как выбрать такой узел. по идее все узлы кор группы имеют одну и туже информацию по членствам и сбоям кластера
* при маппинге узлов узел известен заранее и посылка идет без установки соединения, нужно убедиться, что сбой коннекта в этом случае не будет пропущен и будет детектирован как сбой узла (скорее всего так где-то уже это проходили). Если коре контроллер сменил диспозицию маппинга, то канал будет закрыт как неиспользуемый, мы должны определить эту ситуацию и не детектировать сбой в этом случае, т.е. раз узел не принадлежит стратегии трекинга его сбой игнорируется детектором сбоев. реализовать как для детектора сбоев кор группы так и детектора сбоев кластера
----------
Делаем:
* WorkerToCoreMembershipProvider
* поддержка нескольких адресов для узла и выбор их в канале (возможно один из них главный)
* поддержка в детекторе сбоев учета сбоев только от трекируемых узлов (через хартбит и транспорт)
* проверить discovery
* доделать membership protocols (см выше проблемы)
* доделать failure detector
* WorkerNodeChannel, CoreNodeChannel - каналы рабочего и core узлов
* поддержать для узлов доп. свойства: роли, параметры производительности, метрики и др.
-------------
Создание адреса:
* TcpAddressSerializer - сериализация/десериализация
* TcpChannelHandshaker.deserialize - установка remoteAddress connection'а(connection.setRemoteAddress). InetSocketAddress устанавливается:
  - для серверного соединения из данных, полученных с клиента
  - для клиентского из remoteInetAddress соединения
* TcpAddress.start инициализация локального адреса (TcpTransport.localNode)
  - для сервера - из локального inet адреса сервера
  - для клиента - из локального хоста и нулевого порта
* TcpTransport.createClientConnection инициализация локального адреса  клиентского соединения (connection.setLocalAddress)
  - для сервера - локальный адрес
  - для клиента - локальный адрес tcp канала
* MembershipManagerTests

Использование ITcpAddress.getAddress:
* TcpAddressSerializer - сериализация/десериализация
* TcpChannelHandshaker
  - .disconnect - дисконнект дубликатов
  - .serialize - сериализация инет адреса клиентского соединения
* TcpConnection
  - .dump
  - .setRemoteAddress
* TcpTransport
  - addServerConnection - проверка дубликатов
  - connect, disconnect, dumpConnection, getLastReadTime, getLastWriteTime, lockFlow, unlockFlow, onNodesFailed, onNodesLeft, register, send - поиск соединения и проверка на локальное соединение
  - connect, register, send - создание клиентского соединения
* FailureChannelTests  
  
Использование IAddress.getConnection: 
* добавить  параметр id в getConnection
* LiveNodeManager - start, onNodesConnected, onNodesFailed, onNodesLeft - - редактирование мапки адрес по строке соединения
  - регистрировать для всех строк соединения адреса, они уникальны
* ConnectionManager - connect/disconnect - получение строки 
соединения для определения провайдера коннекторов  
  - ConnectionManager - используется в Channel, ProtocolStack и всех его протоколах для каноникализации и установки соединений
  - нужно связать с transportId
* тесты
  
Transport.localNode:
* в транспорте для установки локального адреса соединения
* для установки локального узла LiveNodeManager'а, который испооьзуется везде в протоколах

TcpConnection
- localAddress - используется при десериализации собщений, при детектировании дубликатов и при сериализации в хендшейкере
- remoteAddress/remoteInetAddress - используется [Todo:]
[TODO: IMessage.getSource/.getDestination - передача в качестве данных другим узлам - вощможны проблемы при использовании клиентскиз адресов, к ним нельзя приконнектиться, нужна серверная версия адреса]