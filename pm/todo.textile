* блокировка/разблокировка формирования членства группы в членстве кластера до момента завершения инсталляции ее локального членства через флаш
* инициация переноса состояния до включения узла в группу (подключения текушего state transfer к группе)
* отладить мультикаст и починить тесты мультикаста
* WorkerNodeChannel, CoreNodeChannel, фабрика подстека группы, стратегия маппинга групп (хотя бы самая простая)
* поддержка штатного выхода рабочего узла из кластера
* уведомление о потере ланных при потере всех узлов группы
* сборка проекта
* тесты
----------
* юнит тест мультикастов не проходит
 - доделать проверку на ресивере при приеме сообщения, что все сообщения с тем же идентификатором приняты на других узлах, если да логгировать этот факт
  - проверять сколько послано и сколько принято
* дописать дефалтовые значения в GroupChannelFactory
* сделать юнит перф тест (взять за основу testMulticast посылать смаксимальной скоростью с учетом флоу контрол, исследовать зависимость пропускной способности и задержек от размера сообщения и параметров фабрики канала)
* Расширить StateTransfer Тесты - сделать тесты эмулирующие различные сбои при работе с состояние (reject, fail при загрузке/выгрузке и др.)
--------------------
Исследования на будущее:
* продумать изоляцию данных от кода, коллоцированных на одном хосте в одном (или разных узлах) с дешевым локальным взаимодействием между ними. Плюс - процессная изоляция, возможность использовать произвольную прикладную платформу (jvm, python, nodejs...)
* продумать про обновление кода - rolling updates, upfront поднятие доп. узла на том же хосте, спараллельным вводом его параллельно, без обновления данных (только код). Обновление частями кластера, например по зонам или по датацентрам
*  распределенный стриминг 
  - DAG обработка данных коллоцированная с самими данными, как узел графа - отдельный этап обработки
  - консистентные снимки, запись маркера в хранилище после определенного этапа обработки, с возможностью дальнейшего восстановления всего снимка для заданного маркера
  - использование окон данных для нарезки бесконечных потоков и обработка данных в рамках текущего окна
* дать возможность разделять на разных узлах узлы с данными и узлы без данных
* можно заменять имя потока на время выполнения запроса и восстанавливать после. Далее на проблемном запросе можно снять дамп потоков и увидеть всю диагностику. Аналогично можно использовать MDC или маркер для записи диагностики
----------
Делаем:
* cluster group, flush
* cluster state transfer, multicast
* поддержать для узлов и групп доп. свойства: роли, параметры производительности, метрики и др.
* вставить логи в там, где нужно
* code review и тесты всего, что сделано
* WorkerNodeChannel, CoreNodeChannel - каналы рабочего и core узлов
  - CoreClusterMembershipProtocol - должен быть выше по стеку чем мультикаст
  - для мультидоменной архитектуры - общий диспатчер и компартмент, разные подканалы со своим транспортом, ssl и менеджером коннектов
----------------------
Группы задачи:
* формирование и изменение глобального членства группы в членстве кластера и ее инсталояция на рабрчие узлы: 
  - стратегии маппинга рабочих узлов на группы и отработка сбоев и выходов узлов
  членство группы, дельта, ищменение, сериалайзер дельты, поддержка свойств, флаги (используемые в протоколах группы - например, мультикасте)
  - уведомление о потере данных при потере всез узлов группы
* инициация переноса состояния до включения узла в группу
* формирование и инсталояция через флаш локального членства группы на рабочих узлах
 - поддержка независимой работы нескольких групп на одном узле
 - при инсталляции членства кластера формирование локальных членств групп иинсталяции их чере флаш (по каждой группе независимо - message stabibization + state transfer)
 - поддержка ограниченной истории локальных групп рабочего узла
 - блокировка/разблокировка формирования членства группы в членстве кластера до момента завершения инсталляции ее локального членства через флаш
* поддержка штатного выхода рабочего узла из кластера
  - посылка запроса на выход и отработка его формированием нового членства группы
  - выход при миграции группы с этого узла
  
* детектирует потерю данных, если группа есть, но старый и новый состав ее узлов не пересекаются (потеря данных детектируется при переносе состояния
* не забыть при перенрсе срстояния в группу кластера дернуть IGroupJoinStrategy
* переинсталляция того же членства кластера на воркерах не должна вызывать повторную инсталляцию членства группы
--------------
«Искусство войны» Сунь Цзы, «Самурае без меча» Масао Китами и «Книге пяти колец» Миямото Мусаси. 

Подробнее на РБК:
http://www.rbc.ru/opinions/business/14/03/2017/58c7cc329a79470adfcec757?utm_source=right_11
--------------
* Если изменение членства групп не блокировать, а выстраивать в очередь и применять последовательно на группе, все вне группы могут получить новое членство группы раньше, чем оно будет инсталлировано в группе и слать в нем сообщения. Тогда группа должна обрабатывать ситуации, когда сообщениешлется как в старом членстве, так и в более новом. С другой стороны членство кластера и членство группы никак не соотнесены по идентификаторам, то что видят все внегруппы может отличаться от того, что установлено в группе
* аналогично перенос состояния и вход узла могут быть инициированы в группе при приходе нового членства кластера с новыми узлами группы. Т.е. несмотря на то, что узел включен в группу в кластере, он не сразу включается в членство группы, а с начала инициируется асинхронный перенос, далее идет включение узла в группу. 
* достоинства: простота - реализуем все локально в группе, не касаясь кластера
* недостатки: все в кластере видят более новое состояние, группы чем есть на самом деле. На сколько это плохо?
--------
Делаем:
* на группе при получении нового членства, если flush in progress на каждом узле группы сохраняем новое членство в очередь. по завершении флаш или по таймеру или при приходе нового членства если текущий - координатор группы исписок ожидающих членств не пустой, начинаем его инсталляцию. удаляем членство из очереди после отработки его флаша. Если координатор падает, мы гарантированно получим новое членство, по событию которого обрабатываем очередь
* новые узлы исключаются из группы, запоминаются и для них инициируется перенос состояния, по завершении переноса узел уведомляет координатора о возможности входа и координатор стартует флаш для входа ущла (возможно нескольких). новый узел удаляется из списка новых после завершения его входа на всех узлах группы. Поскольку все узлы группы имеют идентичное членство кластера, и состав их очередей членств групп и новых узлов одинаковый. Списки редактируются при приходе членства кластера (добавление и инициация установки) и при завершении флаш (удаление из списка), которые выполняются на всех узлах одинаково. Поэтому падение координатора не влияет на общую работоспособность, новый координатор продолжит либо с начала (если старый упал до флаш) либо завершит флаш и продолжит далее
* здесь важно чтобы все операции редактирования списков были идемпотентны
* GroupProtocolSubstack отвечает за все это