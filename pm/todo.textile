* блокировка/разблокировка формирования членства группы в членстве кластера до момента завершения инсталляции ее локального членства через флаш
* инициация переноса состояния до включения узла в группу (подключения текушего state transfer к группе)
* отладить мультикаст и починить тесты мультикаста
* WorkerNodeChannel, CoreNodeChannel, фабрика подстека группы, стратегия маппинга групп (хотя бы самая простая)
* поддержка штатного выхода рабочего узла из кластера
* уведомление о потере ланных при потере всех узлов группы
* сборка проекта
* тесты
----------
* юнит тест мультикастов не проходит
 - доделать проверку на ресивере при приеме сообщения, что все сообщения с тем же идентификатором приняты на других узлах, если да логгировать этот факт
  - проверять сколько послано и сколько принято
* дописать дефалтовые значения в GroupChannelFactory
* сделать юнит перф тест (взять за основу testMulticast посылать смаксимальной скоростью с учетом флоу контрол, исследовать зависимость пропускной способности и задержек от размера сообщения и параметров фабрики канала)
* Расширить StateTransfer Тесты - сделать тесты эмулирующие различные сбои при работе с состояние (reject, fail при загрузке/выгрузке и др.)
--------------------
Исследования на будущее:
* продумать изоляцию данных от кода, коллоцированных на одном хосте в одном (или разных узлах) с дешевым локальным взаимодействием между ними. Плюс - процессная изоляция, возможность использовать произвольную прикладную платформу (jvm, python, nodejs...)
* продумать про обновление кода - rolling updates, upfront поднятие доп. узла на том же хосте, спараллельным вводом его параллельно, без обновления данных (только код). Обновление частями кластера, например по зонам или по датацентрам
*  распределенный стриминг 
  - DAG обработка данных коллоцированная с самими данными, как узел графа - отдельный этап обработки
  - консистентные снимки, запись маркера в хранилище после определенного этапа обработки, с возможностью дальнейшего восстановления всего снимка для заданного маркера
  - использование окон данных для нарезки бесконечных потоков и обработка данных в рамках текущего окна
* дать возможность разделять на разных узлах узлы с данными и узлы без данных
* можно заменять имя потока на время выполнения запроса и восстанавливать после. Далее на проблемном запросе можно снять дамп потоков и увидеть всю диагностику. Аналогично можно использовать MDC или маркер для записи диагностики
----------
Делаем:
* cluster group, flush
* cluster state transfer, multicast
* поддержать для узлов и групп доп. свойства: роли, параметры производительности, метрики и др.
* вставить логи в там, где нужно
* code review и тесты всего, что сделано
* WorkerNodeChannel, CoreNodeChannel - каналы рабочего и core узлов
  - CoreClusterMembershipProtocol - должен быть выше по стеку чем мультикаст
  - для мультидоменной архитектуры - общий диспатчер и компартмент, разные подканалы со своим транспортом, ssl и менеджером коннектов
----------------------
Группы задачи:
* формирование и изменение глобального членства группы в членстве кластера и ее инсталояция на рабрчие узлы: 
  - стратегии маппинга рабочих узлов на группы и отработка сбоев и выходов узлов
  членство группы, дельта, ищменение, сериалайзер дельты, поддержка свойств, флаги (используемые в протоколах группы - например, мультикасте)
  - уведомление о потере данных при потере всез узлов группы
* инициация переноса состояния до включения узла в группу
* формирование и инсталояция через флаш локального членства группы на рабочих узлах
 - поддержка независимой работы нескольких групп на одном узле
 - при инсталляции членства кластера формирование локальных членств групп иинсталяции их чере флаш (по каждой группе независимо - message stabibization + state transfer)
 - поддержка ограниченной истории локальных групп рабочего узла
 - блокировка/разблокировка формирования членства группы в членстве кластера до момента завершения инсталляции ее локального членства через флаш
* поддержка штатного выхода рабочего узла из кластера
  - посылка запроса на выход и отработка его формированием нового членства группы
  - выход при миграции группы с этого узла
  
* детектирует потерю данных, если группа есть, но старый и новый состав ее узлов не пересекаются (потеря данных детектируется при переносе состояния
* не забыть при перенрсе срстояния в группу кластера дернуть IGroupJoinStrategy
* переинсталляция того же членства кластера на воркерах не должна вызывать повторную инсталляцию членства группы
--------------
«Искусство войны» Сунь Цзы, «Самурае без меча» Масао Китами и «Книге пяти колец» Миямото Мусаси. 

Подробнее на РБК:
http://www.rbc.ru/opinions/business/14/03/2017/58c7cc329a79470adfcec757?utm_source=right_11
--------------
WorkGroupDiscoveryProtocol:
* наслелник IGroupMembershipListener, IClusterMembershipListener
* имеет зависимосиь на IGroupJoinStrategy
* на IGroupMembershipListener.onJoin, membershipChanged запоминает текущее членство группы
* на IClusterMembershipListener.onJoin, membershipChanged 
  - проверяет, если членство группы не установлено и текущий узел не координатор будущей группы, шлет координатору запрос о входе (и запоминает координатора). При получении ответа вызывает IGroupJoinStrategy для входа в группу, которая в свою очередь шлет координатору запрос о начале синхронной фазы. При смене членства группы в кластере (повторный вызов этих методов), проверяет, если координатор сменился или не установлен (первый вызов), шлет новому координатору (если не текущий узел) запрос на вход
  - если текущий узел координатор ничего не делать
* если текущий узел координатор 
  - при получении запроса на вход шлет список текущих healthy узлов группы (получая их из детектора сбоев)  
  - при получении запроса на синхронную фазу, запомнить в наборе discovered узлов для предоставления дальше при формировании членства (в качестве IgroupnodeDiscoverer)
----------
GroupProtocolSubstack:
* при установке/изменении членства группы в кластере (на узле координаторе группы):
  - если членство группы не установлено, устанавливает
  - если есть вышедшие/сбойные, формирует новое членство группы, исключив их (они исключаются также из  очереди на вход новых узлов, если они там есть)
  - если есть новые, запоминает список новых в очереди на вход (узлы должны входить списком все, иначе нарушится порядок следования узлов между членствами группы в кластере и в группе)
  - если в текуший момент идет флаш, формирование нового чоенства откоалывается (можно формирование сделать по таймеру, если текущий узел координатор группы, нет текущего флаша и есть сбойные/вышедшие (формируются при приходе членства кластера) или есть полностью заполненные входящие (голова очереди входящиз сравнивается с дрступными через дискаверер)
* периодически дергает IgroupnodeDiscoverer, проверяя доступны ли все узлы списка из головы очереди входящих, если да, включает их в новое членство  

* при падении рабочего узла будет сформировано новое членство кластера, где он будет помечен каксбойный/вышедший, в группе это будет расценено как сбой/выход, и он булет исключен из группы, все процессы  переноса состояния/входа будут отменены
* при падении координатора группы также будет сформировано новое членство группы и новый координатор продолжит формирование членств группы
* если на координаторе группы нет установленного членства группы, а группа не новая, дернуть стратегию потери данных, т.к. данные придеися тянуть из внешнего хранилища и все изменения не созраненные в нем теряются
