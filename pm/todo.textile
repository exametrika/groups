* юнит тест мультикастов не проходит
 - доделать проверку на ресивере при приеме сообщения, что все сообщения с тем же идентификатором приняты на других узлах, если да логгировать этот факт
  - проверять сколько послано и сколько принято
* дописать дефалтовые значения в GroupChannelFactory
* сделать юнит перф тест (взять за основу testMulticast посылать смаксимальной скоростью с учетом флоу контрол, исследовать зависимость пропускной способности и задержек от размера сообщения и параметров фабрики канала)
* Расширить StateTransfer Тесты - сделать тесты эмулирующие различные сбои при работе с состояние (reject, fail при загрузке/выгрузке и др.)
--------------
Base part:
* cluster membership и все что с ним связано (по аналонии с простым)
  - providers (подлержать node membership, workerToCore membership для начала)
* failure detector
  - worker (failure node history, фильтрация сообщений от сбойных, send shun сбойным и reconnect on shun, сбои формируются на узле по новому членству)
  - core 
    * worker node tracking strategy (маппинг берется из членства)
    * composite node tracking (для объединения с core group node tracking)
    * накапливает сбои и посылает core координатору, который формирует новое членство кластера 
    * список сбоев узла корректируется после инсталляции нового членства кластера (удаляя не вошелшие в новое членство)
    * узел ведет текущего координатора, перепосылая сбои в случае его смены
* мультитранспортная архитектура
  - необходима для задания периметров безопасности - формирует ssl периметр по каждому домену,  ограничивает состав сообщений от каждого домена (например, без этого клиент может послать shun core узлу и тот сложится)
  - основная проблема в - сейчас нельзя поддержать несколько транспортов в рамказ одного компартмента, т.к. у каждого свой диспетчер
  - идеальной была бы картина, когда имеем несколько стеков протоколов, каждый со своим транспортом в рамказ одного компартмента. Для этого нужно поддержать общий диспатчер, не зависящий от настроек конкретной ssl фабрики
  - тогда трекинг сбойных велся бы по каждому транспорту свой и не требовалась бы их композиция
  - и еще нужно не запутаться при посылке сообщений между каналами, нужно иметь в протоколе соотв. сендера, а не просто посылать в свой стек
  - сейчас такая возможность есть, просто создаем общего tcp диспетчера и компартмент в канале связи на все транспорты и их стек протоколов
--------------
* CoreClusterMembershipProtocol - должен быть выше по стеку чем мультикаст

Проблемы:
* в мультидоменной архитектуре узлу соответсвует более одного адреса, нужен способ получения правильного адреса в каждом из каналов, все адреса имеют одинаковый идентификатор, равный идентификатору узла
* распространять сбои воркеров сразу и без ожидания через текущего координатора кластера по всем узлам, коректировать при инсталляции членства
* генерировать новое членство кластера только после инсталляции предыдущего, воркеры шлют респонс по инсталляции, коре контроллер шлет координатору
* в алучае падения координатора или коре контроллера мезаника инсталляции членства и распространения сбойных блокируется, другой узел должен взять на себя досылку предыдущего недоинсталлированного членства или сбоев, как выбрать такой узел. по идее все узлы кор группы имеют одну и туже информацию по членствам и сбоям кластера
* при маппинге узлов узел известен заранее и посылка идет без установки соединения, нужно убедиться, что сбой коннекта в этом случае не будет пропущен и будет детектирован как сбой узла (скорее всего так где-то уже это проходили). Если коре контроллер сменил диспозицию маппинга, то канал будет закрыт как неиспользуемый, мы должны определить эту ситуацию и не детектировать сбой в этом случае, т.е. раз узел не принадлежит стратегии трекинга его сбой игнорируется детектором сбоев. реализовать как для детектора сбоев кор группы так и детектора сбоев кластера
----------
Делаем:
* WorkerToCoreMembershipProvider
* поддержка нескольких адресов для узла и выбор их в канале (возможно один из них главный)
* поддержка в детекторе сбоев учета сбоев только от трекируемых узлов (через хартбит и транспорт)
* проверить discovery
* доделать membership protocols (см выше проблемы)
* доделать failure detector
* WorkerNodeChannel, CoreNodeChannel - каналы рабочего и core узлов
* поддержать для узлов доп. свойства: роли, параметры производительности, метрики и др.